{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7d09c8d-0a13-4afe-a5e6-f945e0c27d9b",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7de905a",
   "metadata": {},
   "source": [
    "*** Neural style transfer implemented in Tensorflow, modified for to suit 3D T1s from https://www.tensorflow.org/tutorials/generative/style_transfer ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42787c64-c615-4e96-937d-2f260e135913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "import IPython.display as display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from numba import cuda, jit, njit\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from nilearn import plotting\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import sys\n",
    "import scipy.ndimage\n",
    "import skimage.transform as skTrans\n",
    "\n",
    "from preprocessingfunctions import * #Custom \n",
    "from stylefunctions import * #Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351968d-2235-460a-a948-948f0999fd17",
   "metadata": {},
   "source": [
    "*** Random seed number for repeatability ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191838e-b3b9-4c3d-b19c-0041fec6d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.random.set_seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6d931-10ad-4777-8c5e-f8650f08c759",
   "metadata": {},
   "source": [
    "## Import pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9275f9e-4739-4158-af55-a93a186a856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = keras.models.load_model('model_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108755f0-3d8c-4f14-a820-1cfb92b24779",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8cc35-6b50-4d84-b714-5383684ab7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory_GE = 'insert path' \n",
    "destination_directory_GE = 'insert path'\n",
    "dfGE = pd.read_csv(os.path.join(source_directory_GE,'GE_CO.csv'))\n",
    "dfGE = dfGE.sort_values('filename')\n",
    "dfGE = dfGE.reset_index()\n",
    "\n",
    "source_directory_SE = 'insert path'\n",
    "destination_directory_SE = 'insert path'\n",
    "dfSIEMENS = pd.read_csv(os.path.join(source_directory_SE,'SE_CO.csv'))\n",
    "dfSIEMENS = dfSIEMENS.sort_values('filename')\n",
    "dfSIEMENS = dfSIEMENS.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc7fa7-3532-43d5-926a-dd9c3f5f13c8",
   "metadata": {},
   "source": [
    "## Create arrays for ease of feeding into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cab5b-6213-426a-b2cb-1874b98f22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Content_scans = np.array([process_scan(path) for path in dfGE['FILEPATH']])\n",
    "Style_scans = np.array([process_scan(path) for path in dfSIEMENS['FILEPATH']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935c93e-3b96-4a8a-a52b-78dc2dde8a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View (optional)\n",
    "print(Style_scans.shape)\n",
    "def imshow(image, title=None):\n",
    "    \n",
    "    if len(image.shape) > 3:\n",
    "        image = image[:,:,10]\n",
    "        \n",
    "    image = ndimage.rotate(image, 90)\n",
    "    plt.imshow(image,cmap='jet')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "imshow(Style_scans[0,:,:,75],'a')\n",
    "\n",
    "print(Style_scans[0,:,:,75].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa55b4-d3cd-4cd4-b188-2bef85c50a5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the content and style losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99789855-c026-4681-9a85-6761df9c4269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    \n",
    "    #Style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "    \n",
    "    #Content loss\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    \n",
    "    #Total loss\n",
    "    loss = style_loss + content_loss\n",
    "    return style_loss, content_loss, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b875479-f814-4dd6-98ad-77babb20da42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Clip image dynamic ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40580a71-f174-4dba-9ab7-887221cfafe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    tf.clip_by_value(image, clip_value_min=image.min(), clip_value_max=image.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c998c98-e4a8-46bb-8481-8b184d6d5cb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe592020-7bb0-47c5-a2c9-e52aac1de81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @tf.function()\n",
    "def train_step(Tobe_styled_image,e,m):     \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(Tobe_styled_image)\n",
    "        style_loss, content_loss, loss = style_content_loss(outputs)\n",
    "        tvl = total_variation_loss(Tobe_styled_image)\n",
    "        loss += total_variation_weight*tvl  \n",
    "\n",
    "    grad = tape.gradient(loss, Tobe_styled_image)\n",
    "    opt.apply_gradients([(grad, Tobe_styled_image)])\n",
    "    #----\n",
    "    wandb.log({\"Loss\":loss.numpy()/total_variation_weight,\n",
    "               \"Epochs\":e,\n",
    "               \"Steps per Epoch\":m})\n",
    "    return Tobe_styled_image.assign(Tobe_styled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45498c39-b712-4cf1-9f77-2276de93570e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run style transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba4843-b219-47e5-9856-f1f72886e7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(name='NST_vanila',\n",
    "          project='Neural Style Transfer')\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "import tqdm.notebook as tq\n",
    "from tqdm.auto import tqdm, trange\n",
    "start = time.time()\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "# Set some options here...\n",
    "epochs = 1 \n",
    "steps_per_epoch = 100\n",
    "total_variation_weight = 10e3\n",
    "\n",
    "bar = trange(len(dfGE))\n",
    "for i in bar:\n",
    "    print('Running style transfer on GE scan -->',i+1)\n",
    "    Tobe_styled_image = tf.Variable(Content_scans[i,:,:,:][None,:,:,:])\n",
    "    #  \n",
    "    model_custom = keras.models.load_model(model_path)\n",
    "    prediction_probabilities = model_custom(Tobe_styled_image)\n",
    "    prediction_probabilities.shape\n",
    "\n",
    "    print('------------------------------------------')\n",
    "    #choose content and style layers, refer to output above for the correct layer names\n",
    "    content_layers = ['conv3d'] #The first layer to preserve structural intergrity of the GE scans\n",
    "    style_layers = ['conv3d','conv3d_1','conv3d_2','conv3d_3', 'conv3d_4','conv3d_5','conv3d_6','conv3d_7','conv3d_8',\n",
    "           'conv3d_9','conv3d_10','conv3d_11','conv3d_12','conv3d_13', 'conv3d_14','conv3d_15','conv3d_16','conv3d_17','conv3d_18','conv3d_19'] #Every layer chosen so that maximum style can be extracted\n",
    "    \n",
    "    num_content_layers = len(content_layers)\n",
    "    num_style_layers = len(style_layers)\n",
    "\n",
    "    style_image = Style_scans[i,:,:,:][None,:,:,:]\n",
    "    style_extractor = model_layers(style_layers)\n",
    "    style_outputs = style_extractor(style_image)\n",
    "\n",
    "    print('Number of content layers', num_content_layers)\n",
    "    print('Number of style layers', num_style_layers)\n",
    "\n",
    "    extractor = StyleContentModel(style_layers, content_layers)   \n",
    "    style_targets = extractor(style_image)['style']\n",
    "    content_targets = extractor(Tobe_styled_image)['content']\n",
    "    \n",
    "    print('------------------------------------------')\n",
    "    for e in range(epochs):\n",
    "        # wandb.log({\"Epochs\":e})\n",
    "        for m in tq.tqdm(range(steps_per_epoch)):\n",
    "            train_step(Tobe_styled_image,e,m) #styled image returned here\n",
    "\n",
    "    ni_styled_image = nib.Nifti1Image(np.squeeze(Tobe_styled_image), nib.load(dfGE['FILEPATH'][i]).affine) #GE Styled converetd back to NIFTI format       \n",
    "    end = time.time()\n",
    "    print(\"Total time: {:.1f}\".format(end-start))\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    styled_image_path = 'insert path'\n",
    "    styled_image_name =  dfGE[\"filename\"][i]\n",
    "    nib.save(ni_styled_image, styled_image_path+styled_image_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
